{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65yD30-1ndYY"
   },
   "source": [
    "# COGS 108 - TEAM 44: FINAL PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfhVHH73ndYc"
   },
   "source": [
    "# Overview\n",
    "Today we live in a society where businesses are greatly impacted by customer reviews and the location of where their businesses are. For some businesses, this can result in their success or failure. Looking to improve the experience for owners and customers, we used the yelp’s open-source dataset, _____, and _____ dataset to try to predict and determine what locations are better or worse for the chain Starbucks’ in California. It is important when opening a restaurant or a chain to consider where to open to make the most profit and build an image. Of course, when we look into Starbucks, we can tell that their goal is to open as many stores as they can. With our data analysis, however, we can hopefully give further insight to chains such as Starbucks to help them be the most efficient, even if they want to open as many stores as possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O17HYcG2ndYi"
   },
   "source": [
    "# Group Member IDs and Names \n",
    "\n",
    "U08273331 Baichuan Tang\n",
    "\n",
    "A14394510 Catherine Wang\n",
    "\n",
    "A14881312 Jian Fan\n",
    "\n",
    "A12426296 Kim Pham\n",
    "\n",
    "A15212562 Ryan Eveloff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sWKJfwHndYm"
   },
   "source": [
    "# Research Question\n",
    "\n",
    "Our project sets out to predict *the best location(s) to open the next Starbucks*.\n",
    "And to answer that question, we will be looking at Starbucks store data and how that maps to Census demographics to develop a \"profile\" (list of criteria) of a city where Starbucks is likely to expand its business. Using this \"profile\", we will produce a list of potential cities meeting those criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdiDIlUXndYq"
   },
   "source": [
    "# Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUfRPz1wndYs"
   },
   "source": [
    "Before diving into the real nitty-gritty code work, we did some research to best assist us. First, to stress the importance of our project, we looked at how Starbucks has been employing technology to its favor: in a recent <a href=\"https://www.geekwire.com/2019/microsoft-teams-starbucks-predictive-drive-thru-ordering-bean-cup-blockchain/\">collaboration</a> with Microsoft, they use machine learning to gather customer preferences, connecting coffeemakers in stores to the internet to blockchain services for tracing coffee. They are talking about the increased hiring of software engineers at companies outside the industry that are embracing high tech solutions. This is important for our project because we are using data science to figure out how the relationships and concepts we learn from our data can help a company and franchise like Starbucks. If we are able to find factors that affect and optimize a certain location for Starbucks to open, it can be really useful for companies that can benefit from data scientists and software engineers.\n",
    "Fortunately a <a href=\"https://pdfs.semanticscholar.org/00b4/46b1a605c29326690d8c038c30030b7342cd.pdf\">study</a> by Chungyu Institute of Technology provides us some pointers. It suggested that \"Starbucks Coffee should also pay more attention to male, young customers who are under 35 years old, and consumers who have a high school diploma or equivalent and associate\n",
    "degree for increasing market share.\" Therefore we will be analyzing factors such as gender, age, and education level to see if there's a correlation (if any) between certain demographics, the locations established by Starbucks, and its success in those locations.\n",
    "\n",
    "Additionally, we found a similar <a href=\"https://towardsdatascience.com/analyzing-and-predicting-starbucks-location-strategy-3c5026d31c21?fbclid=IwAR2F6BPuAQfFJU9OXS5WQaSlv2zuB6wPl89WXqU5ORcaPfq-2rtMH-2zOfA\">project</a> in R done by Jordan Bean. He is someone that is naturally curious about how Starbucks think about location selection, and how their store locations can reveal their customer base and how they view locations as more attractive than others. His project is similar to our project in many ways because we also want to look into the locations of Starbucks. However, we want to focus on generating an idea of where Starbucks locations can be and are most successful, not necessarily what Starbucks themselves plan when choosing a location to open their stores. Jordan Bean usefully uses zip codes to differentiate locations of Starbucks just like us too. \n",
    "\n",
    "References (include links):\n",
    "- 1) Starbucks' collaboration with Microsoft: https://www.geekwire.com/2019/microsoft-teams-starbucks-predictive-drive-thru-ordering-bean-cup-blockchain/\n",
    "- 2) Jordan Bean's article: https://towardsdatascience.com/analyzing-and-predicting-starbucks-location-strategy-3c5026d31c21?fbclid=IwAR2F6BPuAQfFJU9OXS5WQaSlv2zuB6wPl89WXqU5ORcaPfq-2rtMH-2zOfA\n",
    "- 3) Study: https://pdfs.semanticscholar.org/00b4/46b1a605c29326690d8c038c30030b7342cd.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4BSBoh_ndYw"
   },
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-_qO9ZOndYx"
   },
   "source": [
    "Our project primarily focuses on determining the relationship between Starbucks success in established locations and associated demographic factors such as gender, age, income, houseprice, and education level. We predict that there will be a positive correlation among the factors stated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSpPepvXndYy"
   },
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3kyTLgXwndYz"
   },
   "source": [
    "*Fill in your dataset information here*\n",
    "\n",
    "(Copy this information for each dataset)\n",
    "- Dataset Name:\n",
    "- Link to the dataset:\n",
    "- Number of observations:\n",
    "\n",
    "1-2 sentences describing each dataset. \n",
    "\n",
    "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMSy9FfendY0"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W88Hov3qndY1"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# \n",
    "# Note: these are all the imports you need for this assignment!\n",
    "# Do not import any other functions / packages\n",
    "\n",
    "# Display plots directly in the notebook instead of in a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Configure libraries\n",
    "# The seaborn library makes plots look nicer\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "\n",
    "# Don't display too many rows/cols of DataFrames\n",
    "pd.options.display.max_rows = 7\n",
    "pd.options.display.max_columns = 8\n",
    "\n",
    "# Round decimals when displaying DataFrames\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHCtV5M9ndY6"
   },
   "source": [
    "# 1) Data Cleaning\n",
    "Data files:\n",
    "    - Starbucks store_data.csv\n",
    "    - Census "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) Starbucks: Load data\n",
    "Import data into a DataFrame structure called `starbucks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wU5OTkujndY-"
   },
   "outputs": [],
   "source": [
    "starbucks=pd.read_csv('store_data.csv')\n",
    "starbucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an analysis of the data and seeing that the majority of Starbucks is in the US, we decided to direct our focus to predicting a US location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top10 countries which has the largest number of starbucks\n",
    "top10country=starbucks.groupby('Country').apply(lambda x:len(x)).sort_values(ascending=False)[:10]\n",
    "\n",
    "plt.pie(top10country.values,labels=top10country.keys(),autopct='%.0f%%',\n",
    "        textprops = {'fontsize':12, 'color':'k'},shadow=True)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Top 10 countries which has the largest number of starbucks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrow down all Starbucks to only Starbucks in the US and store the data in `df_US`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US=starbucks[starbucks['Country']=='US']\n",
    "print(\"There are %s Starbucks in the US\"%len(df_US))\n",
    "df_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table have the following columns\n",
    "df_US.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrow down even more to have to table only contain information we need. In other words, drop columns \"Brand\" (obviously they're all Starbucks), \"Store Number\", \"Ownership Type\", \"Country\", \"Phone Number\", and \"Timezone\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US = df_US.drop(columns=['Brand','Store Number','Ownership Type','Country','Phone Number','Timezone'])\n",
    "#reseting index to start from 0\n",
    "df_US.reset_index(drop=True,inplace=True)\n",
    "#Rename some columns to have simpler name\n",
    "df_US=df_US.rename({'State/Province':'State','Postcode':'zip'}, axis='columns')\n",
    "df_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) Starbucks: Find missing data\n",
    "Drop rows with missing data. We will try to minimize the number of rows dropped to maximize our prediction accuracy. Therefore we will only drop rows with missing data in the following columns \"Store Number\", \"Ownership Type\", \"City\", \"State\", and \"Zip\" because they are of importance to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US = df_US.dropna(subset=['City','State','zip'])\n",
    "df_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c) Starbucks: Standardize zip codes\n",
    "We can see that some zips have 9 digits and some have 5. We will standardize them to have 5 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US.loc[:,'zip']=pd.Series([str(i)[:5] for i in df_US.loc[:,'zip']])\n",
    "df_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standarding the zips, some end up with only 4 digits because python omits the first 0. Therefore, we will manually prepend 0 in front of 4-digit zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df_US.iterrows():\n",
    "    zip = str(row['zip'])\n",
    "    if len(zip)<5:\n",
    "        #create new zip\n",
    "        newzip = zip.zfill(5)\n",
    "        #change in df_US accordingly\n",
    "        df_US.at[index,'zip'] = newzip\n",
    "df_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is purely for checking if our code above works\n",
    "#The cell should output a five-digit zip \n",
    "df_US['zip'].loc[3985]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d) Census: Importing census package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before importing packages, we must first install census, us, and censusdata APIs. The following code box is commented out because we only needed to install the APIs once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install census\n",
    "#!pip install us\n",
    "#!pip install censusdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from census import Census\n",
    "from us import states\n",
    "import censusdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Census(\"d05cc8c102603c7b50d7705c8f45251177e6346b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of more meaningful titles to later replace the existing census data variables. For example, census data variable \"B01001_008E\" maps to \"Estimate Total Male 20 years,\" according to this <a href=\"https://api.census.gov/data/2016/acs/acs1/variables.html\">index</a>, and therefore will be replaced with title \"male_20\" as seen below.\n",
    "\n",
    "As stated in our hypothesis, we are only interested in gender, age, income, houseprice, and education level factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = dict(male_15_17 = 'B01001_006E',\n",
    "male_18_19 = 'B01001_007E',\n",
    "male_20 = 'B01001_008E',\n",
    "male_21 = 'B01001_009E',\n",
    "male_22_24 = 'B01001_010E',\n",
    "male_25_29 = 'B01001_011E',\n",
    "male_30_34 = 'B01001_012E',\n",
    "male_35_39 = 'B01001_013E',\n",
    "male_40_44 = 'B01001_014E',\n",
    "male_45_49 = 'B01001_015E',\n",
    "male_50_54 = 'B01001_016E',\n",
    "male_55_59 = 'B01001_017E',\n",
    "male_60_61 = 'B01001_018E',\n",
    "male_62_64 = 'B01001_019E',\n",
    "male_65_66 = 'B01001_020E',\n",
    "male_67_69 = 'B01001_021E',\n",
    "male_70_74 = 'B01001_022E',\n",
    "male_75_79 = 'B01001_023E',\n",
    "male_80_84 = 'B01001_024E',\n",
    "male_85_plus = 'B01001_025E',\n",
    "female_15_17 = 'B01001_030E',\n",
    "female_18_19 = 'B01001_031E',\n",
    "female_20 = 'B01001_032E',\n",
    "female_21 = 'B01001_033E',\n",
    "female_22_24 = 'B01001_034E',\n",
    "female_25_29 = 'B01001_035E',\n",
    "female_30_34 = 'B01001_036E',\n",
    "female_35_39 = 'B01001_037E',\n",
    "female_40_44 = 'B01001_038E',\n",
    "female_45_49 = 'B01001_039E',\n",
    "female_50_54 = 'B01001_040E',\n",
    "female_55_59 = 'B01001_041E',\n",
    "female_60_61 = 'B01001_042E',\n",
    "female_62_64 = 'B01001_043E',\n",
    "female_65_66 = 'B01001_044E',\n",
    "female_67_69 = 'B01001_045E',\n",
    "female_70_74 = 'B01001_046E',\n",
    "female_75_79 = 'B01001_047E',\n",
    "female_80_84 = 'B01001_048E',\n",
    "female_85_plus = 'B01001_049E',\n",
    "median_age = 'B01002_001E', \n",
    "median_hh_income = 'B19013_001E',\n",
    "male_workers = 'B23022_003E',\n",
    "female_workers = 'B23022_027E',\n",
    "total_population = 'B01001_001E', \n",
    "median_rent = 'B25031_001E',\n",
    "median_home_value = 'B25077_001E',\n",
    "high_school_diploma = 'B15003_017E',\n",
    "bachelors_degree = 'B15003_022E',\n",
    "masters_degree = 'B15003_023E')\n",
    "print(\"There are totally %s raw features selected\"%len(info_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put data into a DataFrame structure called `df_census`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census=pd.DataFrame(c.acs5.zipcode(list(info_dict.values()), Census.ALL))\n",
    "#because the dataframe's columns' order was not by what we provide('info_dict.values()'), but the order of alphabet\n",
    "#So we need to re-order it first\n",
    "cols=list(info_dict.values())\n",
    "cols.append('zip code tabulation area')\n",
    "df_census =df_census[cols]\n",
    "df_census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace census data variables with the dictionary of more meaningful titles we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic=dict(info_dict)\n",
    "#reverse dict\n",
    "dic={v:k for k,v in dic.items()}\n",
    "df_census.rename(dic,axis='columns',inplace=True)\n",
    "df_census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNm7EHqundZC"
   },
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Analysis and Results, we must install geopandas, lippysal, mapclassify, and dill. The following code box is commented out because we only needed to install them once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas\n",
    "#!pip install libpysal\n",
    "#!pip install mapclassify\n",
    "#!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pysal\n",
    "import mapclassify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating relative numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'age < 18'  are seen as outliers since they are less likely to sit in a Starbucks\n",
    "df_census['male_18_24']=np.sum(df_census.iloc[:,1:5],axis=1)\n",
    "df_census['male_25_29']=np.sum(df_census.iloc[:,5:6],axis=1)\n",
    "df_census['male_30_39']=np.sum(df_census.iloc[:,6:8],axis=1)\n",
    "df_census['male_40_49']=np.sum(df_census.iloc[:,8:10],axis=1)\n",
    "df_census['male_50_plus']=np.sum(df_census.iloc[:,10:20],axis=1)\n",
    "df_census['female_18_24']=np.sum(df_census.iloc[:,21:25],axis=1)\n",
    "df_census['female_25_29']=np.sum(df_census.iloc[:,25:26],axis=1)\n",
    "df_census['female_30_39']=np.sum(df_census.iloc[:,26:28],axis=1)\n",
    "df_census['female_40_49']=np.sum(df_census.iloc[:,28:30],axis=1)\n",
    "df_census['female_50_plus']=np.sum(df_census.iloc[:,30:40],axis=1)\n",
    "\n",
    "#The original format of total_population was str, which cannot be used to calculate\n",
    "df_census['total_population']=df_census['total_population'].apply(float)\n",
    "df_census['percent_18_24']=(df_census['male_18_24']+df_census['female_18_24'])/df_census['total_population']\n",
    "df_census['percent_25_29']=(df_census['male_25_29']+df_census['female_25_29'])/df_census['total_population']\n",
    "df_census['percent_30_39']=(df_census['male_30_39']+df_census['female_30_39'])/df_census['total_population']\n",
    "df_census['percent_40_49']=(df_census['male_40_49']+df_census['female_40_49'])/df_census['total_population']\n",
    "df_census['percent_50_plus']=(df_census['male_50_plus']+df_census['female_50_plus'])/df_census['total_population']\n",
    "\n",
    "#Here we want to see whether there is a significant diference between genders,\n",
    "#so we make three seperate percentages\n",
    "df_census['total_workers']=df_census['male_workers']+df_census['female_workers']\n",
    "df_census['percent_male_workers']=df_census['male_workers']/df_census['total_population']\n",
    "df_census['percent_female_workers']=df_census['female_workers']/df_census['total_population']\n",
    "df_census['percent_workers']=df_census['total_workers']/df_census['total_population']\n",
    "\n",
    "#The following is used to calculate the propotion of high school/bachelor's/master's degree among different age groups\n",
    "df_census['total_18_plus']=(np.sum(df_census.iloc[:,4:23],axis=1)+np.sum(df_census.iloc[:,24:43],axis=1))\n",
    "df_census['total_22_plus']=(np.sum(df_census.iloc[:,7:23],axis=1)+np.sum(df_census.iloc[:,27:43],axis=1))\n",
    "df_census['total_25_plus']=(np.sum(df_census.iloc[:,8:23],axis=1)+np.sum(df_census.iloc[:,28:43],axis=1))\n",
    "df_census['percent_highschool']=df_census['high_school_diploma']/df_census['total_18_plus']\n",
    "df_census['percent_bachelor']=df_census['bachelors_degree']/df_census['total_22_plus']\n",
    "df_census['percent_master']=df_census['masters_degree']/df_census['total_25_plus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAbDZC53ndZE"
   },
   "source": [
    "Here we get our final census Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census=df_census[['zip code tabulation area','total_population','median_age',\n",
    "                     'median_hh_income','median_rent','median_home_value',\n",
    "                     'percent_18_24','percent_25_29','percent_30_39','percent_40_49',\n",
    "                     'percent_50_plus','total_workers','percent_male_workers',\n",
    "                     'percent_female_workers','percent_workers',\n",
    "                     'percent_highschool','percent_bachelor','percent_master']]\n",
    "df_census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify whether a zip code location in US has Starbucks, so we compare the zipcodes in the census Dataframe with that in US Starbucks Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_census.index:\n",
    "    amount=sum(df_US['zip']==df_census.loc[i,'zip code tabulation area'])\n",
    "    if amount>0:\n",
    "        df_census.loc[i,'has_starbucks']=1\n",
    "        df_census.loc[i,'starbucks_amount']=amount\n",
    "    else:\n",
    "        df_census.loc[i,'has_starbucks']=0\n",
    "        df_census.loc[i,'starbucks_amount']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are totally %s columns containing NaNs.\"%sum(df_census.isnull().any(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we find that there is some NaNs in the census Dataframe, we still wants to ploy the choropleth map first before droping these rows. This is because the number of rows containing Nans (317 rows) are very small compared to the total amount (33120 rows). Also, missing those rows with a specific zip code would show nothing on the map, which is not advisible.\n",
    "Use an inherient Geopandas DataFrame and merge it with the US Starbucks Dataframe df_US, thus we can later make choropleth maps based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_table = gpd.read_file(libpysal.examples.get_path('us48.shp'))\n",
    "df_US=df_US.merge(geo_table,left_on='State',right_on='STATE_ABBR')\n",
    "df_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census.rename({'zip code tabulation area':'zip'},axis='columns',inplace=True)\n",
    "df = pd.merge(df_census, df_US, how='left', on=['zip'])\n",
    "pd.options.display.max_columns=100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=gpd.GeoDataFrame(df)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1,figsize = (25,12))\n",
    "df.plot(ax=ax,column='total_workers', cmap='OrRd', scheme='quantiles', legend=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jordanbean/Starbucks-Location-Mapping/blob/master/starbucks_r_script_upd.Rmd\n",
    "\n",
    "http://darribas.org/notebooks/dynamics/giddy/Rank_based_Methods#Regional-exchange-mobility-pattern-in-US-1929-2009\n",
    "\n",
    "https://github.com/pysal/libpysal/blob/master/libpysal/examples/us_income/usjoin.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = world[(world.pop_est>0) & (world.name!=\"Antarctica\")]\n",
    "world['gdp_per_cap'] = world.gdp_md_est / world.pop_est\n",
    "world.plot(column='gdp_per_cap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopandas.datasets.available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Drop nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B20XcMObndZI"
   },
   "source": [
    "# Ethics & Privacy\n",
    "\n",
    "__Personal privacy__: While we are sifting through customer data, their acceptance of the Yelp EULA (End-User License Agreement) gives us the opportunity to use their data. \n",
    "\n",
    "__Eliminate biases__: Our project aims to determine and clarify the traits in the Starbucks that had more business. And by no means do we subject any Starbucks to bias. \n",
    "\n",
    "__The potential of misusing the results__: The results of our research are not meant for advertisements to make money or sabotaging the images of established restaurants that do not possess the characteristics concluded from our project. Our project might positively influence the direction in which the restaurant heads for. Always believe that Practice Makes Perfect.\n",
    "\n",
    "__Special conditions__: Some Yelp reviews may not be accurate, such as an angry customer writing a scathing review and blowing their experience out of proportion, which would lead to some noise in our data. Therefore, to the best of our abilities we will be removing such observations (outliers, noise, etc) from our dataset with clear indication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FesFl5SnndZL"
   },
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PF8B6Z_endZN"
   },
   "source": [
    "*Fill in your discussion information here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FinalProject.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
